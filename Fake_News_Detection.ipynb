{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPu6ICRvS8or"
      },
      "source": [
        "**Author:**  M. Adil Fayyaz\n",
        "\n",
        "**Email:** adilfayyaz6@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpPzrrPtSNw-"
      },
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0KGCkzoTMCZ"
      },
      "source": [
        "# File Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iM-7_MoI_l5"
      },
      "source": [
        "## **Read the Training Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUAVI4MgMTRk"
      },
      "source": [
        "Reading the training set of documents and created a dictionary named Train which stores all the data in the the Training dataset provided. The key values of the dictionary are 'Real' and 'Fake' which classify the real news from the fake news.\n",
        "\n",
        "The data is preprocessed by removing the punctuation marks from the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBzJKXCaTO7S"
      },
      "source": [
        "# Mount the Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5/Train/Real\n",
        "# Train Set Dictionary\n",
        "Train = {}\n",
        "RealData = [[]]\n",
        "FakeData = [[]]\n",
        "\n",
        "punctuation1 = ['‘','؟','،','!',\"'\",' ؔ','َ','ِ','ُ','“','\"','”','%','٪','۔',':','(',')','/','|','\\\\','\\n','.','*','&','^','$','=','+','@','{','}','[',']']\n",
        "\n",
        "for doc in os.listdir(os.getcwd()):\n",
        "    with open(os.path.join(os.getcwd(), doc), 'r') as filereading: \n",
        "      lines = filereading.readlines()\n",
        "    for i, val in enumerate(lines):\n",
        "      for x in punctuation1:\n",
        "        lines[i] = lines[i].replace(x,\" \")\n",
        "    RealData.append(lines)\n",
        "\n",
        "%cd /content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5/Train/Fake\n",
        "for doc in os.listdir(os.getcwd()):\n",
        "    with open(os.path.join(os.getcwd(), doc), 'r') as filereading: \n",
        "      lines = filereading.readlines()\n",
        "    for i, val in enumerate(lines):\n",
        "      for x in punctuation1:\n",
        "        lines[i] = lines[i].replace(x,\" \")\n",
        "    \n",
        "    FakeData.append(lines)\n",
        "\n",
        "Train['Real'] = RealData\n",
        "Train['Fake'] = FakeData\n",
        "Train['Real'].pop(0)\n",
        "Train['Fake'].pop(0)\n",
        "Train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vNaIHwcI7VR"
      },
      "source": [
        "## **Read the Testing Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUaHlqzfNJfT"
      },
      "source": [
        "Reading the training set of documents and created a dictionary named Test which stores all the data in the the Testing dataset provided. The key values of the dictionary are 'Real' and 'Fake' which classify the real news from the fake news, to allow us to use them as the true labels and be able to evaluate the Naive Bayes Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEaXqOroG9a_"
      },
      "source": [
        "# Read the Testing Files\n",
        "%cd /content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5/Test/Real\n",
        "# Test Set Dictionary\n",
        "Test = {}\n",
        "RealData2 = [[]]\n",
        "FakeData2 = [[]]\n",
        "for doc in os.listdir(os.getcwd()):\n",
        "    with open(os.path.join(os.getcwd(), doc), 'r') as f: \n",
        "      lines = f.readlines()\n",
        "    for i, val in enumerate(lines):\n",
        "      for x in punctuation1:\n",
        "        lines[i] = lines[i].replace(x,\" \")\n",
        "      \n",
        "    RealData2.append(lines)\n",
        "\n",
        "%cd /content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5/Test/Fake\n",
        "for doc in os.listdir(os.getcwd()):\n",
        "    with open(os.path.join(os.getcwd(), doc), 'r') as f: \n",
        "      lines = f.readlines()\n",
        "    for i, val in enumerate(lines):\n",
        "      for x in punctuation1:\n",
        "        lines[i] = lines[i].replace(x,\" \")\n",
        "    FakeData2.append(lines)\n",
        "    \n",
        "Test['Real'] = RealData2\n",
        "Test['Fake'] = FakeData2\n",
        "Test['Real'].pop(0)\n",
        "Test['Fake'].pop(0)\n",
        "Test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMu-1q4GI5Zy"
      },
      "source": [
        "## **Read the Stopwords File**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lLGxtmlNa8W"
      },
      "source": [
        "Read the stop words file and store all the stopping words into a list called stopWords.\n",
        "\n",
        "Also remove the stop words from the testing set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dij-wcFRJJRm",
        "outputId": "e26fe9f2-5da6-4934-a7cf-f8ac0b921201"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5\n",
        "stopWords = []\n",
        "with open('stopwords-ur.txt','r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "for i, value in enumerate(lines):\n",
        "  val = value.replace('\\n',\"\")\n",
        "  stopWords.append(val)\n",
        "\n",
        "TestRemovedStop = copy.deepcopy(Test)\n",
        "# Remove the stopping words before testing\n",
        "Classes = {'Real','Fake'}\n",
        "for c in Classes:\n",
        "  for i, val in enumerate(TestRemovedStop[c]):\n",
        "    for j, sentence_list in enumerate(TestRemovedStop[c][i]):\n",
        "      words = sentence_list.split()\n",
        "      new_words = []\n",
        "      for w in words:\n",
        "        if w not in stopWords:\n",
        "          new_words.append(w)\n",
        "      sent = \"\"\n",
        "      for w in new_words:\n",
        "        sent += w\n",
        "        sent += \" \"\n",
        "      TestRemovedStop[c][i][j] = sent\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP Spring 2021/Assignments/Assignment 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgpe1JvD5iMb"
      },
      "source": [
        "removeStopWords Function takes as parameter the Training set and the classes and removes the stop words from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lQz7YI45MI9"
      },
      "source": [
        "def removeStopWords(TrainS1, Classes):\n",
        "  # Remove the stopping words before training\n",
        "  for c in Classes:\n",
        "    for i, val in enumerate(TrainS1[c]):\n",
        "      for j, sentence_list in enumerate(TrainS1[c][i]):\n",
        "        words = sentence_list.split()\n",
        "        new_words = []\n",
        "        for w in words:\n",
        "          if w not in stopWords:\n",
        "            new_words.append(w)\n",
        "        sent = \"\"\n",
        "        for w in new_words:\n",
        "          sent += w\n",
        "          sent += \" \"\n",
        "        TrainS1[c][i][j] = sent\n",
        "\n",
        "  return TrainS1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477aP-Kuaoow"
      },
      "source": [
        "# Multinomial Naive Bayes Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_pbD4o2A-Zi"
      },
      "source": [
        "### Helping Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHr4y_UgNkps"
      },
      "source": [
        "Function countTexts takes as parameter the Training set and the Classes i.e. Real of Fake. It Iterates over the classes and returns the total number of documents read in the Training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g7Khyidc_r_"
      },
      "source": [
        "# Return the total number of documents/texts read\n",
        "def countTexts(Train, Classes):\n",
        "  sumValue = 0\n",
        "  for i in Classes:\n",
        "    sumValue =sumValue +len(Train[i])\n",
        "  return sumValue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZvMN-2vN9zx"
      },
      "source": [
        "Function extractVocabulary takes as parameter the Training set and the Classes. It returns a list of words - Vocab, which contains the unique words existing in the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaJg_UNvfBTz"
      },
      "source": [
        "def extractVocabulary(Train, Classes):\n",
        "  Vocab = []\n",
        "  for i in Classes:\n",
        "    for j in Train[i]:\n",
        "      for sentence in j:  \n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "          if word not in Vocab:\n",
        "            Vocab.append(word)\n",
        "          \n",
        "  return Vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVZR8kR1OWL8"
      },
      "source": [
        "Function countTextsInClass returns the length of documents in a particular class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgnqOzAgsF7"
      },
      "source": [
        "def countTextsInClass(Train, c):\n",
        "  return len(Train[c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPiGVZm0Ob5K"
      },
      "source": [
        "Function countWordsInAllTextsOfClass takes as parameter the Training set and a class. It iterates over the training dictionary belonging from the class passed as a parameter in the function. It returns the number of words in all the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvFjHqrYg7-X"
      },
      "source": [
        "# Returns the number of words in a class over all the texts\n",
        "def countWordsInAllTextsOfClass(Train, c):\n",
        "  count = 0 \n",
        "  for i in Train[c]:\n",
        "    for sentence in i:\n",
        "      words = sentence.split()\n",
        "      for word in words:\n",
        "        count += 1  \n",
        "\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn80KNsYO8Ez"
      },
      "source": [
        "Function concatenateTextsInClass takes as parameter the training set and a class. It returns a single document which contains all the words in the documents combined into a single document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX8np5Khiimy"
      },
      "source": [
        "# get all the texts of a particular class in a single document\n",
        "def concatenateTextsInClass(Train, c):\n",
        "  concatText = []\n",
        "  for i in Train[c]:\n",
        "    for sentence in i:\n",
        "      words = sentence.split()\n",
        "      for word in words:\n",
        "        concatText.append(word)\n",
        "\n",
        "  return concatText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_cxKNN8PYQE"
      },
      "source": [
        "Function countTokensOfWords takes as parameter a single document and a word. It returns the number of times that word occurs in the document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Cg_258loeI"
      },
      "source": [
        "# return number of times w appears in doc\n",
        "def countTokensOfWords(docC, w):\n",
        "  count = 0\n",
        "  for word in docC:\n",
        "    if word == w:\n",
        "      count += 1\n",
        "\n",
        "  return count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kFGt_CXBGra"
      },
      "source": [
        "## Naive Bayes Training Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUwlwdKn3sC0"
      },
      "source": [
        "Naive Bayes Training Algorithm using the helping functions defined above. In this algorithm we first extracted the vocabulary - a set of words, from our training set. Then, we got the total count of documents in our training set. Then we iterate over our list of Classes i.e. Real and Fake, and we get the number of documents in a class and the count of words in all the documents of a class. For every word in the vocabulary we can then calculate the token of words and then eventually the conditional probability. As we iterate over all the classes we are then able to calculate the conditional probability for all the words and calculate the prior values for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAOuS1J_Vr9E"
      },
      "source": [
        "def NaiveBayesTraining(Train,Classes):\n",
        "  Vocab = extractVocabulary(Train,Classes)\n",
        "  totalN_val = countTexts(Train, Classes)\n",
        "  conditionalProb = {}\n",
        "  priorC = {}\n",
        "  for c in Classes:\n",
        "    Nc = countTextsInClass(Train, c)\n",
        "    Nw = countWordsInAllTextsOfClass(Train, c)\n",
        "    priorC[c] = float(Nc/totalN_val)\n",
        "    # get all texts of a particular class in a single document\n",
        "    docC = concatenateTextsInClass(Train, c)\n",
        "    \n",
        "    for wi in Vocab:\n",
        "      Ni = countTokensOfWords(docC, wi)\n",
        "      conditionalProb[(wi,c)] = (Ni+1)/(Nw+len(Vocab))\n",
        "\n",
        "  return Vocab, priorC, conditionalProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8BmibaYBMyy"
      },
      "source": [
        "## Boolean Naive Bayes Training Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sevJunAv4Jjb"
      },
      "source": [
        "Takes as parameter the Boolean Training Set preprocessed and returns the prior values and conditional probabilities for each word belonging from a class. In the Boolean Naive Bayes Training class duplicate words are removed from the training set and then the priors and conditional probabilities are returned from the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSkqSQ_P4hdP"
      },
      "source": [
        "def BooleanNaiveBayesTraining(TrainBoolean, Classes):\n",
        "  return NaiveBayesTraining(TrainBoolean, Classes)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV88XGlHFW4w"
      },
      "source": [
        "## Training both the Models\n",
        "With and Without Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55-zFS0egbfh"
      },
      "source": [
        "**Naive Bayes Training - Without Stop Words Removal**\n",
        " \n",
        " In this Training, the Naive Bayes Training Algorithm is used without removing the stop words provided in the stop words file \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn6SRQReXCZx"
      },
      "source": [
        "Classes = {'Real','Fake'}\n",
        "def NaiveBayes(Train):\n",
        "  return NaiveBayesTraining(Train, Classes)\n",
        "V, priorC, conditionalProb = NaiveBayes(Train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3HRwNq2giQB"
      },
      "source": [
        "**Boolean Naive Bayes Training - Without Stop Words Removal**\n",
        "\n",
        "In this Training, the Boolean Naive Bayes Algorithm is used. The only difference is, that in this training the duplicate words in the file are removed. The stop words provided in the stop words file are not removed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU_8Dsbpebz1"
      },
      "source": [
        "def BoolNaiveBayes(Train):\n",
        "  TrainC = copy.deepcopy(Train)\n",
        "  # Remove duplicate words in each document\n",
        "  for c in Classes:\n",
        "    for i,val in enumerate(TrainC[c]):\n",
        "      wordsUnique = []\n",
        "      for j, sentence_list in enumerate(TrainC[c][i]):\n",
        "        words = sentence_list.split()\n",
        "        for word in words:\n",
        "          if word not in wordsUnique:\n",
        "            wordsUnique.append(word)\n",
        "\n",
        "      sentence_form = \"\"\n",
        "      sentence_form_list = []\n",
        "    \n",
        "      # Converting words list to sentence form, requirement of the training function\n",
        "      for uword in wordsUnique:\n",
        "        sentence_form += uword\n",
        "        sentence_form += \" \"\n",
        "      sentence_form_list.append(sentence_form) \n",
        "      TrainC[c][i] = sentence_form_list\n",
        "\n",
        "  Vbool, priorCbool, condProbBool = BooleanNaiveBayesTraining(TrainC, Classes)\n",
        "  return TrainC, Vbool, priorCbool, condProbBool\n",
        "\n",
        "TrainC, Vbool, priorCbool, condProbBool = BoolNaiveBayes(Train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6XaeGZhBn5"
      },
      "source": [
        "**Naive Bayes Training - Removing Stop Words**\n",
        "\n",
        "In this Training, the Naive Bayes Training Algorithm is used and the stop words provided in the stop words file is also removed from the training set (and later the testing set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGwKfNdyCWwB"
      },
      "source": [
        "def NaiveBayesStop(Train):\n",
        "  Classes = {'Real','Fake'}\n",
        "  TrainS1 = copy.deepcopy(Train)\n",
        "  TrainS1 = removeStopWords(TrainS1,Classes)\n",
        "  VStop, priorCStop, conditionalProbStop = NaiveBayesTraining(TrainS1, Classes)\n",
        "  return TrainS1, VStop, priorCStop, conditionalProbStop\n",
        "TrainS1, VStop, priorCStop, conditionalProbStop = NaiveBayesStop(Train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qr84eelhXYD"
      },
      "source": [
        "**Boolean Naive Bayes - Removing Stop Words**\n",
        "\n",
        "In this Training, the Boolean Naive Bayes Algorithm is used, (duplicate words removed) while also removing the stop words, provided in the stop words file,from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDcUIndpRmYQ"
      },
      "source": [
        "def BoolNaiveBayesStop(TrainC):\n",
        "  Classes = {'Real','Fake'}\n",
        "  TrainS2 = copy.deepcopy(TrainC)\n",
        "  TrainS2 = removeStopWords(TrainS2,Classes)\n",
        "  VStopBool, priorCStopBool, condProbStopBool = BooleanNaiveBayesTraining(TrainS2, Classes)\n",
        "  return TrainS2, VStopBool, priorCStopBool, condProbStopBool\n",
        "TrainS2, VStopBool, priorCStopBool, condProbStopBool = BoolNaiveBayesStop(TrainC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBei3VRdDr_h"
      },
      "source": [
        "# **Application of Naive Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkjPM8X4heJy"
      },
      "source": [
        "### Naive Bayes Test - Find Score Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9z2SiWo6Z9B"
      },
      "source": [
        "Function extractWordsFromText takes as parameter the Vocabulary and the text. It returns a list of words in the text that belong from/exist in the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_IgGTiEhCoK"
      },
      "source": [
        "def extractWordsFromText(Vocab , text):\n",
        "  words_list = []\n",
        "  \n",
        "  sentence_form = \"\"\n",
        "  xwords = text.split()\n",
        "  for w in xwords:\n",
        "    sentence_form += w\n",
        "    sentence_form += ' '\n",
        "  words =sentence_form.split()\n",
        "  for w in words:\n",
        "    if w in Vocab: \n",
        "      words_list.append(w)\n",
        "\n",
        "  return words_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PTQhRKD69h5"
      },
      "source": [
        "Function NaiveBayesTest takes as parameter the classes, the vocabulary, the prior values, the conditional probability and text from the Testing set. \n",
        "\n",
        "The function calculates the log score values for each of our word found in the vocabulary. The log scores are computed from the conditional probability of the words that were calculated in the Naive Bayes Algorithm above.\n",
        "\n",
        "The function returns the maximum score class that is either Real or Fake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR03SKXg8MUW"
      },
      "source": [
        "def NaiveBayesTest(Classes, V, prior, condprob, text):\n",
        "  W = extractWordsFromText(V, text)\n",
        "  scoreValue = {}\n",
        "  for c in Classes:\n",
        "    scoreValue[c] = math.log(prior[c])\n",
        "    for word in W:\n",
        "        scoreValue[c] += math.log(condprob[(word,c)])\n",
        "      \n",
        "   \n",
        "  return max(scoreValue, key=scoreValue.get)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UlUtya7ETAg"
      },
      "source": [
        "### Evaluation Without Stop Words Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1xySS_ZGyf9"
      },
      "source": [
        "Evaluation of Naive Bayes without stop word removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ClIQZ0SFrXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82350eb4-cc46-4917-8e17-1da5edfb4c12"
      },
      "source": [
        "# Naive Bayes Only\n",
        "\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for c in Classes:\n",
        "  for q, val in enumerate(Test[c]):\n",
        "    s = \"\"\n",
        "    for sentence_list in val:\n",
        "      s += sentence_list\n",
        "      s += \" \"   \n",
        "    recv_class = NaiveBayesTest(Classes, V, priorC, conditionalProb,s)\n",
        "    predicted_labels.append(recv_class)\n",
        "    correct_labels.append(c)\n",
        "   \n",
        "print(\"Accuracy is: \" , accuracy_score(correct_labels, predicted_labels, normalize = True))\n",
        "print(\"Precision is: \", precision_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall is: \", recall_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"F1 Score is: \", f1_score(correct_labels, predicted_labels, average=\"macro\"))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.6946564885496184\n",
            "Precision is:  0.6877104377104377\n",
            "Recall is:  0.6858333333333333\n",
            "F1 Score is:  0.6866028708133971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5wj-HtqG1g-"
      },
      "source": [
        "Evaluation of Boolean Naive Bayes without stop word removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M06urMIiQWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edac0145-645c-4948-db39-bd4f7e686f9a"
      },
      "source": [
        "# Boolean Naive Bayes Only\n",
        "\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for c in Classes:\n",
        "  for q, val in enumerate(Test[c]):\n",
        "    s = \"\"\n",
        "    for sentence_list in val:\n",
        "      s += sentence_list\n",
        "      s += \" \"\n",
        "    recv_class = NaiveBayesTest(Classes, Vbool, priorCbool, condProbBool,s)\n",
        "    predicted_labels.append(recv_class)\n",
        "    correct_labels.append(c)\n",
        "    \n",
        "\n",
        "\n",
        "print(\"Accuracy is: \" , accuracy_score(correct_labels, predicted_labels, normalize = True))\n",
        "print(\"Precision is: \", precision_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall is: \", recall_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"F1 Score is: \", f1_score(correct_labels, predicted_labels, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.7137404580152672\n",
            "Precision is:  0.7094250043959909\n",
            "Recall is:  0.7126785714285715\n",
            "F1 Score is:  0.710189814609973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FN69H_yqGHT"
      },
      "source": [
        "Here, as the evaluations can be seen the Boolean Naive Bayes version (with duplicates removal) seems to perform better than the normal Naive Bayes Version. The reason being that the total number of words in each class become less so the conditional probability will only be considering if a certain word exists in a particular class or not. With that logic, the Boolean version performs better and shows a better result because it classifies words into classes and learns if a certain word belongs from that class or not rather than considering the number of times it occurs in a particular document. \n",
        "\n",
        "It is important to note here that even though removing duplicate words (in the Boolean version) is showing a better accuracy result, in fact this technique is actually rather ambigious. In this case it showed better results, however, if we remove the duplicate words from the training set, we are actually disturbing the context of sentences in our training data. The disturbance in the context will not be conclusive results or observations, in fact it will only add to the ambiguity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3JpIfV9IZak"
      },
      "source": [
        "### Evaluation With Stop Words Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "que3qVCCh_DR"
      },
      "source": [
        "Evaluation of Naive Bayes Algorithm with stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMK3K5YnROv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242dd0dd-a31b-438c-967b-18a35ab21288"
      },
      "source": [
        "# Naive Bayes with stop words removed Only\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for c in Classes:\n",
        "  for q, val in enumerate(TestRemovedStop[c]):\n",
        "    s = \"\"\n",
        "    for sentence_list in val:\n",
        "      s += sentence_list\n",
        "      s += \" \"   \n",
        "    recv_class = NaiveBayesTest(Classes, VStop, priorCStop, conditionalProbStop,s)\n",
        "    predicted_labels.append(recv_class)\n",
        "    correct_labels.append(c)\n",
        "\n",
        "print(\"Accuracy is: \" , accuracy_score(correct_labels, predicted_labels, normalize = True))\n",
        "print(\"Precision is: \", precision_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall is: \", recall_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"F1 Score is: \", f1_score(correct_labels, predicted_labels, average=\"macro\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.7022900763358778\n",
            "Precision is:  0.6958928571428571\n",
            "Recall is:  0.6958928571428571\n",
            "F1 Score is:  0.6958928571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlF0sbbGiKqA"
      },
      "source": [
        "Evaluation of Boolean Naive Bayes Algorithm with stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGllnnurRX6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c26dba-001e-4972-f1b4-8fa0b663cf4d"
      },
      "source": [
        "# Boolean Naive Bayes with stop words removed Only\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for c in Classes:\n",
        "  for q, val in enumerate(TestRemovedStop[c]):\n",
        "    s = \"\"\n",
        "    for sentence_list in val:\n",
        "      s += sentence_list\n",
        "      s += \" \"   \n",
        "    recv_class = NaiveBayesTest(Classes, VStopBool, priorCStopBool, condProbStopBool,s)\n",
        "    predicted_labels.append(recv_class)\n",
        "    correct_labels.append(c)\n",
        "\n",
        "print(\"Accuracy is: \" , accuracy_score(correct_labels, predicted_labels, normalize = True))\n",
        "print(\"Precision is: \", precision_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall is: \", recall_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"F1 Score is: \", f1_score(correct_labels, predicted_labels, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.7213740458015268\n",
            "Precision is:  0.7152665347484559\n",
            "Recall is:  0.7136904761904762\n",
            "F1 Score is:  0.7143795827546555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Q_e50ey5_Y"
      },
      "source": [
        "Here, with the stop words removed from the Training set, we can see that the stop word removal resulted in a better accuracy, precision, recall and F1 score as compared to the normal naive bayes version. This is because stop words are high frequency words in a document that add unnecessary requirements on the classifier, in terms of both space and time complexity. In other words, removing stop words helps us classify the more important root words existing in the Fake and Real new documents, removing abundant words from the documents lets us classify those root words in their true essence. Similarly, the model also shows an increase in evaluation metrics with the removal of stop words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8flg91RBjrc"
      },
      "source": [
        "## Evaluation using Negation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEcB6ycv-Wup"
      },
      "source": [
        "Function NegationWordsTrain removes the words following negation words from the training set. It must be noted here that the logic employed was that every word that follows a negation word is removed from the list of words in the training set. I experimented using a list of negation words, some of which are commented out below, and came to the conclusion that when only the single word 'نہیں' is removed from the training set, the accuracy and precision increases as compared to my previous Naive Bayes attempt. In contrast, when I increase the set of negation words, my accuracy is not improved. However, it is still likely if the the negation words set is improved (added more proficient words) the model may show an increase in terms of accuracy and precision. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntgIlAnHBnJZ"
      },
      "source": [
        "# Extra Credit - Remove Negation Words\n",
        "def NegationWordsTrain(TrainS2):\n",
        "  negation_words = ['نہیں'] #'غیر', 'کبھی', 'اندازن', 'بمشکل', 'مشکل']\n",
        "  Classes = {'Real','Fake'}\n",
        "  Train_Negation = copy.deepcopy(TrainS2)\n",
        "  # Remove the stopping words before training\n",
        "  for c in Classes:\n",
        "    for i, val in enumerate(Train_Negation[c]):\n",
        "      for j, sentence_list in enumerate(Train_Negation[c][i]):\n",
        "        words = sentence_list.split()\n",
        "        new_words = []\n",
        "        \n",
        "        for w in range(len(words)):\n",
        "          try:\n",
        "            if words[w] in negation_words:\n",
        "              # print(w+1)\n",
        "              del words[w+1]\n",
        "            new_words.append(words[w])\n",
        "          except:\n",
        "            xyz = 1\n",
        "        sent = \"\"\n",
        "        for w in new_words:\n",
        "          sent += w\n",
        "          sent += \" \"\n",
        "        Train_Negation[c][i][j] = sent\n",
        "  VN, priorCN, conditionalProbN = BooleanNaiveBayesTraining(Train_Negation, Classes)\n",
        "  return Train_Negation, VN, priorCN, conditionalProbN\n",
        "Train_Negation, VN, priorCN, conditionalProbN = NegationWordsTrain(TrainS2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEvqGUXP_CAX"
      },
      "source": [
        "Evaluation of the Negation Training (stop words also removed) \n",
        "\n",
        "Adds accuracy when results compared to previous attempts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbdwdr_aDxd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128d2b8c-4eab-4f75-e670-0e120a6acddf"
      },
      "source": [
        "# Naive Bayes Negation \n",
        "\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "for c in Classes:\n",
        "  for q, val in enumerate(TestRemovedStop[c]):\n",
        "    s = \"\"\n",
        "    for sentence_list in val:\n",
        "      s += sentence_list\n",
        "      s += \" \"   \n",
        "    recv_class = NaiveBayesTest(Classes, VN, priorCN, conditionalProbN,s)\n",
        "    predicted_labels.append(recv_class)\n",
        "    correct_labels.append(c)\n",
        "   \n",
        "print(\"Accuracy is: \" , accuracy_score(correct_labels, predicted_labels, normalize = True))\n",
        "print(\"Precision is: \", precision_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall is: \", recall_score(correct_labels, predicted_labels, average='macro'))\n",
        "print(\"F1 Score is: \", f1_score(correct_labels, predicted_labels, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is:  0.7251908396946565\n",
            "Precision is:  0.7192159692159692\n",
            "Recall is:  0.7170238095238095\n",
            "F1 Score is:  0.7179425837320574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk4CD5dX9s3u"
      },
      "source": [
        "The results of accuracy, precision, recall and f1 scores show an increase as compared to any of the previous results. This is because negation words are difficult to classify. By using negation words, the context of an entire sentence can change, and that is why if we remove the words following the negation words, it will be easier to classify the documents, because there will be less room left for any ambiguity. "
      ]
    }
  ]
}